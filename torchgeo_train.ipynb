{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MAR190Aszv8r"
      },
      "source": [
        "# Torchgeo Первый эксперимент"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0lZeGJNTz1y5"
      },
      "source": [
        "## Введение"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VXG-oXGUz39X"
      },
      "source": [
        "Цель этого ноутбука - попробовать применить torchgeo к нашему созданному датасету:\n",
        "* Создание наборов растровых данных, загрузчиков данных и сэмплеров для изображений и масок;\n",
        "* Набор данных о пересечениях;\n",
        "* Нормализация данных;\n",
        "* Создание спектрального индекса;\n",
        "* Создание модели сегментации (DeepLabV3);\n",
        "* Функция потерь и показатели; и\n",
        "* Цикл обучения.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A14-syGAFahE",
        "outputId": "d5b0ac1d-5cc2-4532-b2e6-7134638e9389"
      },
      "outputs": [],
      "source": [
        "# для колаба\n",
        "# !pip install rasterio -q\n",
        "# !pip install torchgeo -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Кирилл\\Documents\\GitHub\\CV_ITMO_2023\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cuda is available: True\n",
            "PyTorch version: 1.13.1+cu117\n",
            "**********\n",
            "_CUDA version: \n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Jun__8_16:59:34_Pacific_Daylight_Time_2022\n",
            "Cuda compilation tools, release 11.7, V11.7.99\n",
            "Build cuda_11.7.r11.7/compiler.31442593_0\n",
            "**********\n",
            "CUDNN version: 8500\n",
            "Available GPU devices: 1\n",
            "Device Name: NVIDIA GeForce GTX 1650 with Max-Q Design\n"
          ]
        }
      ],
      "source": [
        "# Проверка cuda и ее доступности\n",
        "import torch\n",
        "print(f'Cuda is available: {torch.cuda.is_available()}')\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print('*'*10)\n",
        "print(f'_CUDA version: ')\n",
        "!nvcc --version\n",
        "print('*'*10)\n",
        "print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
        "print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
        "print(f'Device Name: {torch.cuda.get_device_name()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73l1wc-fFWuU"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yY7mCdoq00Yo"
      },
      "source": [
        "Датасет мы создали ранее, его можно найти в папке `data/output/for_torchgeo_way/`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pe9DXvX6HgLf"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9Ia_3fJCFryP"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from typing import Optional, Iterable, List, Callable\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import rasterio as rio\n",
        "\n",
        "from torchgeo.datasets import RasterDataset, unbind_samples, stack_samples\n",
        "from torchgeo.samplers import RandomGeoSampler, Units\n",
        "from torchgeo.transforms import indices\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CYHuH3IoFpX7"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "# this line is to avoid warnings from Rasterio to be printed on screen\n",
        "# the warning is explained here: https://stackoverflow.com/questions/74089170/suppress-rasterio-warning-warning-1-tiffreaddirectory\n",
        "logger = logging.getLogger(\"rasterio\")\n",
        "logger.setLevel(logging.ERROR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Gs3nB3Qhy8nl"
      },
      "outputs": [],
      "source": [
        "root = Path('./data/output/for_torchgeo_way')\n",
        "assert root.exists()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "abnT63f1GOh8"
      },
      "source": [
        "## Создание датасета"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7ke5sQ1_4nEq"
      },
      "source": [
        "Нужно подготовить его к загрузке в нейронную сеть. Для этого мы создадим экземпляр класса Raster Dataset, предоставляемого TorchGeo, и укажем на определенный каталог. Функция `scale` применит масштабирование - `1e-4`, (увидел у одного ученого проверим, нужно ли это делать). Как только наборы данных созданы, мы можем комбинировать изображения с масками (метками), используя оператор `&`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iXiPVrjXGSes"
      },
      "outputs": [],
      "source": [
        "def scale(item: dict):\n",
        "    item['image'] = item['image'] / 10000\n",
        "    return item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yWApUYVYZ1Lr"
      },
      "outputs": [],
      "source": [
        "#res - resolution, типа масштаб снимка, 10 значит 10м в пикселе, как есть в сентинеле\n",
        "train_imgs = RasterDataset(root=(root/'img_f/train').as_posix(), crs='epsg:32637', res=10, transforms=scale) \n",
        "train_msks = RasterDataset(root=(root/'mask_f/train').as_posix(), crs='epsg:32637', res=10)\n",
        "\n",
        "valid_imgs = RasterDataset(root=(root/'img_f/val').as_posix(), crs='epsg:32637', res=10, transforms=scale)\n",
        "valid_msks = RasterDataset(root=(root/'mask_f/val').as_posix(), crs='epsg:32637', res=10)\n",
        "\n",
        "# IMPORTANT\n",
        "train_msks.is_image = False\n",
        "valid_msks.is_image = False\n",
        "\n",
        "train_dset = train_imgs & train_msks\n",
        "valid_dset = valid_imgs & valid_msks\n",
        "\n",
        "train_sampler = RandomGeoSampler(train_imgs, size=256, length=128, units=Units.PIXELS)\n",
        "valid_sampler = RandomGeoSampler(valid_imgs, size=256, length=64, units=Units.PIXELS)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MaTZ03eJ5FGa"
      },
      "source": [
        "CRS (Coordinate Reference System) - TorchGeo требует, чтобы все изображения были с одинаковой системой координат."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TqNOU7WaOJ2t"
      },
      "source": [
        "### Понимание sampler -а"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8RFrF3bTOSJn"
      },
      "source": [
        "Чтобы создать патчи для обучения, которые могут быть загружены в нейронную сеть из нашего набора данных, нам нужно выбрать выборки фиксированных размеров. Torch Geo имеет много сэмплеров, но мы попробуем класс `RandomGeoSampler`. По сути, сэмплер выбирает случайные ограничивающие рамки фиксированного размера, которые принадлежат исходному изображению. Затем эти ограничивающие рамки используются в `RasterDataset` для запроса нужной нам части изображения. Пример с использованием ранее созданных сэмплеров ниже."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8VGVIWNPI_W",
        "outputId": "5b779cd3-25bc-4ec4-e29d-99391e906a4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BoundingBox(minx=306202.95137286186, maxx=308762.95137286186, miny=6347919.420487881, maxy=6350479.420487881, mint=0.0, maxt=9.223372036854776e+18)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bbox = next(iter(train_sampler))\n",
        "bbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFCssvlqPI87",
        "outputId": "4bb06a55-375e-4c14-d790-2429d4f14d0a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['crs', 'bbox', 'image', 'mask'])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample = train_dset[bbox]\n",
        "sample.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-gF-8k1PI6N",
        "outputId": "60002cb9-8d8c-4e1e-ba18-51edf4191ea3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(torch.Size([4, 256, 256]), torch.Size([1, 256, 256]))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample['image'].shape, sample['mask'].shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dGiqU2TcPcTq"
      },
      "source": [
        "Обратите внимание, что теперь у нас есть участки одинакового размера (..., 512 x 512)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TBNfy4X5Pn-G"
      },
      "source": [
        "## Создание Dataloaders"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a97dvCYVP5D5"
      },
      "source": [
        "Создание `DataLoader` в TorchGeo очень простое, такое же, как и в Pytorch (фактически используется тот же класс). Обратите внимание ниже, что мы используем те же sampler, которые ранее задали. Дополнительно мы сообщаем dataset, который загрузчик данных будет использовать для извлечения данных, batch_size (количество выборок в каждом пакете) и функцию сортировки, которая определяет, как “объединить” несколько выборок в один пакет.\n",
        "\n",
        "Наконец, мы можем выполнить итерацию через загрузчик данных, чтобы извлекать из него пакеты. Чтобы протестировать это, давайте получим первую партию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhWa0SYfav2V",
        "outputId": "6e80207e-30cf-46b8-d507-e15c0619aa9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(dict_keys(['crs', 'bbox', 'image', 'mask']),\n",
              " dict_keys(['crs', 'bbox', 'image', 'mask']))"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataloader = DataLoader(train_dset, sampler=train_sampler, batch_size=4, collate_fn=stack_samples)\n",
        "valid_dataloader = DataLoader(valid_dset, sampler=valid_sampler, batch_size=4, collate_fn=stack_samples)\n",
        "\n",
        "train_batch = next(iter(train_dataloader))\n",
        "valid_batch = next(iter(valid_dataloader))\n",
        "train_batch.keys(), valid_batch.keys()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o7VRAzpkQIvr"
      },
      "source": [
        "## Batch визуализация"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "78IKqlWUQMTM"
      },
      "source": [
        "Теперь, когда мы можем извлекать пакеты из наших наборов данных, давайте создадим функцию для отображения пакетов.\n",
        "\n",
        "Функция `plot_batch` автоматически проверит количество элементов в пакете и наличие связанных масок, чтобы соответствующим образом упорядочить выходную сетку."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zynH3etxQQmG"
      },
      "outputs": [],
      "source": [
        "# from typing import Iterable, List, Optional, Callable\n",
        "# import torch\n",
        "\n",
        "# def plot_imgs(images: Iterable, axs: Iterable, chnls: List[int] = [0, 1, 2], bright: float = 5.):\n",
        "#     for img, ax in zip(images, axs):\n",
        "#         arr = torch.clamp(bright * img, min=0, max=1).numpy()\n",
        "#         rgb = arr.transpose(1, 2, 0)[:, :, chnls]\n",
        "#         ax.imshow(rgb)\n",
        "#         ax.axis('off')\n",
        "\n",
        "\n",
        "# def plot_msks(masks: Iterable, axs: Iterable):\n",
        "#     for mask, ax in zip(masks, axs):\n",
        "#         ax.imshow(mask.squeeze().numpy(), cmap='Greens')\n",
        "#         ax.axis('off')\n",
        "\n",
        "# def plot_batch(batch: dict, bright: float = 5., cols: int = 4, width: int = 5, chnls: List[int] = [0, 1, 2]):\n",
        "\n",
        "#     # Get the samples and the number of items in the batch\n",
        "#     samples = unbind_samples(batch.copy())\n",
        "    \n",
        "#     # if batch contains images and masks, the number of images will be doubled\n",
        "#     n = 2 * len(samples) if ('image' in batch) and ('mask' in batch) else len(samples)\n",
        "\n",
        "#     # calculate the number of rows in the grid\n",
        "#     rows = n//cols + (1 if n%cols != 0 else 0)\n",
        "\n",
        "#     # create a grid\n",
        "#     _, axs = plt.subplots(rows, cols, figsize=(cols*width, rows*width))  \n",
        "\n",
        "#     if ('image' in batch) and ('mask' in batch):\n",
        "#         # plot the images on the even axis\n",
        "#         plot_imgs(images=map(lambda x: x['image'], samples), axs=axs.reshape(-1)[::2], chnls=chnls, bright=bright) \n",
        "\n",
        "#         # plot the masks on the odd axis\n",
        "#         plot_msks(masks=map(lambda x: x['mask'], samples), axs=axs.reshape(-1)[1::2]) \n",
        "\n",
        "#     else:\n",
        "\n",
        "#         if 'image' in batch:\n",
        "#             plot_imgs(images=map(lambda x: x['image'], samples), axs=axs.reshape(-1), chnls=chnls, bright=bright) \n",
        "    \n",
        "#         elif 'mask' in batch:\n",
        "#             plot_msks(masks=map(lambda x: x['mask'], samples), axs=axs.reshape(-1)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "0S4DZa8aQd8Z",
        "outputId": "755d6f7a-d4fd-4cab-ec1d-c293255a7e8c"
      },
      "outputs": [],
      "source": [
        "# plot_batch(train_batch)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SkGQoaWlQVFC"
      },
      "source": [
        "## Стандартизация данных и спектральный индекс"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mvce_wKMQ3pT"
      },
      "source": [
        "Обычно методы машинного обучения (включая глубокое обучение) выигрывают от масштабирования функций. Это означает стандартное отклонение около 1 и нулевое среднее значение, применяя следующую формулу:\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "Чтобы сделать это, нам нужно сначала найти среднее значение и стандартное отклонение для каждого из 4х каналов в наборе данных.\n",
        "\n",
        "Давайте определим функцию, вычисляющую эти статистические данные, и запишем ее результаты в переменные mean и std. Мы будем использовать наш ранее установленный пакет rasterio, чтобы открыть изображения и выполнить простое усреднение статистики для каждого пакета / канала. Для стандартного отклонения этот метод является приближенным. Для более точного расчета, пожалуйста, обратитесь к: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.htm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IZ5yXZPzNIdh"
      },
      "outputs": [],
      "source": [
        "def calc_statistics(dset: RasterDataset):\n",
        "        \"\"\"\n",
        "        Calculate the statistics (mean and std) for the entire dataset\n",
        "        Warning: This is an approximation. The correct value should take into account the\n",
        "        mean for the whole dataset for computing individual stds.\n",
        "        For correctness I suggest checking: http://notmatthancock.github.io/2017/03/23/simple-batch-stat-updates.html\n",
        "        \"\"\"\n",
        "\n",
        "        # To avoid loading the entire dataset in memory, we will loop through each img\n",
        "        # The filenames will be retrieved from the dataset's rtree index\n",
        "        files = [item.object for item in dset.index.intersection(dset.index.bounds, objects=True)]\n",
        "\n",
        "        # Reseting statistics\n",
        "        accum_mean = 0\n",
        "        accum_std = 0\n",
        "\n",
        "        for file in files:\n",
        "            img = rio.open(file).read()/10000 #type: ignore\n",
        "            accum_mean += img.reshape((img.shape[0], -1)).mean(axis=1)\n",
        "            accum_std += img.reshape((img.shape[0], -1)).std(axis=1)\n",
        "\n",
        "        # at the end, we shall have 2 vectors with lenght n=chnls\n",
        "        # we will average them considering the number of images\n",
        "        return accum_mean / len(files), accum_std / len(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QH8obaQlQgoe"
      },
      "outputs": [],
      "source": [
        "class MyNormalize(torch.nn.Module):\n",
        "    def __init__(self, mean: List[float], stdev: List[float]):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mean = torch.Tensor(mean)[:, None, None]\n",
        "        self.std = torch.Tensor(stdev)[:, None, None]\n",
        "\n",
        "    def forward(self, inputs: dict):\n",
        "\n",
        "        x = inputs[..., : len(self.mean), :, :]\n",
        "\n",
        "        # if batch\n",
        "        if inputs.ndim == 4:\n",
        "            x = (x - self.mean[None, ...]) / self.std[None, ...]\n",
        "\n",
        "        else:\n",
        "            x = (x - self.mean) / self.std\n",
        "\n",
        "        inputs[..., : len(self.mean), :, :] = x\n",
        "\n",
        "        return inputs\n",
        "\n",
        "    def revert(self, inputs: dict):\n",
        "        \"\"\"\n",
        "        De-normalize the batch.\n",
        "\n",
        "        Args:\n",
        "            inputs (dict): Dictionary with the 'image' key\n",
        "        \"\"\"\n",
        "\n",
        "        x = inputs[..., : len(self.mean), :, :]\n",
        "\n",
        "        # if batch\n",
        "        if x.ndim == 4:\n",
        "            x = inputs[:, : len(self.mean), ...]\n",
        "            x = x * self.std[None, ...] + self.mean[None, ...]\n",
        "        else:\n",
        "            x = x * self.std + self.mean\n",
        "\n",
        "        inputs[..., : len(self.mean), :, :] = x\n",
        "\n",
        "        return inputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4VtubMAPxXYq"
      },
      "outputs": [],
      "source": [
        "normalize = MyNormalize(*calc_statistics(train_imgs))\n",
        "\n",
        "tfms = torch.nn.Sequential(\n",
        "    indices.AppendNDVI(index_nir=3, index_red=0),\n",
        "    normalize\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVsDAOaVRhiN",
        "outputId": "b128640d-b3bc-4cf6-82e1-187b5df2a164"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "transformed_batch = tfms(train_batch['image'])\n",
        "print(transformed_batch.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HA-jTEKeRuA4"
      },
      "source": [
        "Note that our transformed batch has now 5 channels, instead of 4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWfUOS1RR14g"
      },
      "source": [
        "> Important: the normalize method we created will apply the normalization just to the original bands and it will ignore the previously appended indices. That’s important to avoid errors due to distinct shapes between the batch and the mean and std vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfNmPcwWSNFv"
      },
      "source": [
        "## Segmentation Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMmXdpZWSSNR"
      },
      "source": [
        "For the semantic segmentation model, we are going to use a predefined architecture that is available in Pytorch. Looking at list (https://pytorch.org/vision/stable/models.html#semantic-segmentation) it is possible to note 3 models available for semantic segmentation, but one (LRASPP) is intended for mobile applications. In our tutorial, we will use the DeepLabV3 model.\n",
        "\n",
        "Here, we will create a DeepLabV3 model for 2 classes. In this case, I will skip the pretrained weights, as the weights represent another domain (not water segmentation from multispectral imagery)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzhOtV3IyubJ",
        "outputId": "b7e57773-1f4c-4140-9039-b0e84e59aa58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DeepLabV3(\n",
              "  (backbone): IntermediateLayerGetter(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): DeepLabHead(\n",
              "    (0): ASPP(\n",
              "      (convs): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "        (1): ASPPConv(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "        (2): ASPPConv(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "        (3): ASPPConv(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU()\n",
              "        )\n",
              "        (4): ASPPPooling(\n",
              "          (0): AdaptiveAvgPool2d(output_size=1)\n",
              "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (3): ReLU()\n",
              "        )\n",
              "      )\n",
              "      (project): Sequential(\n",
              "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU()\n",
              "        (3): Dropout(p=0.5, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (3): ReLU()\n",
              "    (4): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchvision.models.segmentation import deeplabv3_resnet50\n",
        "model = deeplabv3_resnet50(weights=None, num_classes=2)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvsK_LXuSmnL"
      },
      "source": [
        "The first thing we have to pay attention in the model architecture is the number of channels expected in the first convolution (Conv2d), that is defined as 3. That’s because the model is prepared to work with RGB images. After the first convolution, the 3 channels will produce 64 channels in lower resolution, and so on. As we have now 9 channels, we will change this first processing layer to adapt correctly to our model. We can do this by replacing the first convolutional layer for a new one, by following the commands. Finally, we check a mock batch can pass through the model and provide the output with 2 channels (water / no_water) as desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYPdGBPOSfHJ",
        "outputId": "7e9267b7-a4e7-428d-a0cd-9292dd2b1923"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 256, 256])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "backbone = model.get_submodule('backbone')\n",
        "\n",
        "conv = torch.nn.modules.conv.Conv2d(\n",
        "    in_channels=5, \n",
        "    out_channels=64, \n",
        "    kernel_size=(7, 7),\n",
        "    stride=(2, 2),\n",
        "    padding=(3, 3),\n",
        "    bias=False\n",
        ")\n",
        "backbone.register_module('conv1', conv)\n",
        "\n",
        "pred = model(torch.randn(3, 5, 256, 256))\n",
        "pred['out'].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvvGWtMrTdCE"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuUPhlzxTftk"
      },
      "source": [
        "The training function should receive the number of epochs, the model, the dataloaders, the loss function (to be optimized) the accuracy function (to assess the results), the optimizer (that will adjust the parameters of the model in the correct direction) and the transformations to be applied to each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "sFlsT8glSq3x"
      },
      "outputs": [],
      "source": [
        "def train_loop(\n",
        "    epochs: int, \n",
        "    train_dl: DataLoader, \n",
        "    val_dl: Optional[DataLoader], \n",
        "    model: torch.nn.Module, \n",
        "    loss_fn: Callable, \n",
        "    optimizer: torch.optim.Optimizer, \n",
        "    acc_fns: Optional[List]=None, \n",
        "    batch_tfms: Optional[Callable]=None\n",
        "):\n",
        "    # size = len(dataloader.dataset)\n",
        "    cuda_model = model.cuda()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        accum_loss = 0\n",
        "        for batch in train_dl:\n",
        "\n",
        "            if batch_tfms is not None:\n",
        "                batch['image'] = batch_tfms(batch['image'])\n",
        "\n",
        "            X = batch['image'].cuda()\n",
        "            y = batch['mask'].type(torch.long).cuda()\n",
        "            pred = cuda_model(X)['out']\n",
        "            loss = loss_fn(pred, y)\n",
        "\n",
        "            # BackProp\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # update the accum loss\n",
        "            accum_loss += float(loss) / len(train_dl)\n",
        "\n",
        "        # Testing against the validation dataset\n",
        "        if acc_fns is not None and val_dl is not None:\n",
        "            # reset the accuracies metrics\n",
        "            acc = [0.] * len(acc_fns)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in val_dl:\n",
        "\n",
        "                    if batch_tfms is not None:\n",
        "                        batch['image'] = batch_tfms(batch['image'])                    \n",
        "\n",
        "                    X = batch['image'].type(torch.float32).cuda()\n",
        "                    y = batch['mask'].type(torch.long).cuda()\n",
        "\n",
        "                    pred = cuda_model(X)['out']\n",
        "\n",
        "                    for i, acc_fn in enumerate(acc_fns):\n",
        "                        acc[i] = float(acc[i] + acc_fn(pred, y)/len(val_dl))\n",
        "\n",
        "            # at the end of the epoch, print the errors, etc.\n",
        "            print(f'Epoch {epoch}: Train Loss={accum_loss:.5f} - Accs={[round(a, 3) for a in acc]}')\n",
        "        else:\n",
        "\n",
        "            print(f'Epoch {epoch}: Train Loss={accum_loss:.5f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUIPlgndUB_9"
      },
      "source": [
        "## Loss and Accuracy Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs5LOs8LUS7j"
      },
      "source": [
        "For the loss function, normally the Cross Entropy Loss should work, but it requires the mask to have shape (N, d1, d2). In this case, we will need to squeeze our second dimension manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "O8K3e5NwTrYg"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import jaccard_score\n",
        "\n",
        "def oa(pred, y):\n",
        "    flat_y = y.squeeze()\n",
        "    flat_pred = pred.argmax(dim=1)\n",
        "    acc = torch.count_nonzero(flat_y == flat_pred) / torch.numel(flat_y)\n",
        "    return acc\n",
        "\n",
        "def iou(pred, y):\n",
        "    flat_y = y.cpu().numpy().squeeze()\n",
        "    flat_pred = pred.argmax(dim=1).detach().cpu().numpy()\n",
        "    return jaccard_score(flat_y.reshape(-1), flat_pred.reshape(-1), zero_division=1.)    \n",
        "\n",
        "def loss(p, t):    \n",
        "    return torch.nn.functional.cross_entropy(p, t.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UW6YfmnyUa1A"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xajn1unSUh3h"
      },
      "source": [
        "> To train the model it is important to have CUDA GPUs available. In Colab, it can be done by changing the runtime type and re-running the notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import gc\n",
        "# gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moU9ol78UUqP",
        "outputId": "a1b5af92-b7fe-43c0-c4cd-f92906a46fb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss=nan - Accs=[0.997, 0.188]\n",
            "Epoch 1: Train Loss=nan - Accs=[0.996, 0.188]\n",
            "Epoch 2: Train Loss=nan - Accs=[0.995, 0.188]\n",
            "Epoch 3: Train Loss=nan - Accs=[0.997, 0.188]\n",
            "Epoch 4: Train Loss=nan - Accs=[0.998, 0.125]\n",
            "Epoch 5: Train Loss=nan - Accs=[0.997, 0.312]\n",
            "Epoch 6: Train Loss=nan - Accs=[0.998, 0.25]\n",
            "Epoch 7: Train Loss=nan - Accs=[0.996, 0.188]\n",
            "Epoch 8: Train Loss=nan - Accs=[0.998, 0.375]\n",
            "Epoch 9: Train Loss=nan - Accs=[0.997, 0.438]\n",
            "Epoch 10: Train Loss=nan - Accs=[0.997, 0.25]\n",
            "Epoch 11: Train Loss=nan - Accs=[0.996, 0.312]\n",
            "Epoch 12: Train Loss=nan - Accs=[0.996, 0.312]\n",
            "Epoch 13: Train Loss=nan - Accs=[0.996, 0.188]\n",
            "Epoch 14: Train Loss=nan - Accs=[0.996, 0.312]\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.01)\n",
        "train_loop(15, train_dataloader, valid_dataloader, model, loss, optimizer, \n",
        "           acc_fns=[oa, iou], batch_tfms=tfms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "earthsurfacewater_torchgeo_tutorial.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
